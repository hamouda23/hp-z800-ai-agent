# √âtape 4 : Installation et Configuration du Client PC (Msty)

## üéØ Objectif

Installer et configurer Msty sur votre PC pour avoir une interface graphique moderne pour discuter avec Mistral.

## ‚è±Ô∏è Dur√©e Estim√©e

5-10 minutes

## üìã Pr√©requis

- ‚úÖ Ollama install√© et fonctionnel sur le serveur Z800
- ‚úÖ Mistral t√©l√©charg√©
- ‚úÖ Acc√®s r√©seau configur√© (√âtape 3 compl√©t√©e)
- ‚úÖ IP du serveur connue (exemple: `192.168.1.108`)

---

## üé® Qu'est-ce que Msty ?

**Msty** est un client graphique moderne pour interagir avec des mod√®les LLM locaux comme Ollama.

### ‚úÖ Avantages
- Interface utilisateur √©l√©gante et intuitive
- Support multi-mod√®les
- Historique des conversations
- Personnalisation des prompts
- Gratuit et l√©ger
- Compatible Windows, Mac, Linux

---

## üíª Installation de Msty

### Windows

**M√©thode 1 : Winget (Recommand√©)**
```powershell
# Ouvrir PowerShell
winget install Msty
```

**M√©thode 2 : T√©l√©chargement Direct**
1. Visitez : https://msty.app
2. Cliquez sur **Download for Windows**
3. Ex√©cutez le fichier `.exe`
4. Suivez l'assistant d'installation

---

### macOS

**M√©thode 1 : Homebrew**
```bash
brew install --cask msty
```

**M√©thode 2 : T√©l√©chargement Direct**
1. Visitez : https://msty.app
2. Cliquez sur **Download for Mac**
3. Ouvrez le fichier `.dmg`
4. Glissez Msty dans Applications

---

### Linux

**AppImage (Toutes distributions)**
1. Visitez : https://msty.app
2. T√©l√©chargez le fichier `.AppImage`
3. Rendez-le ex√©cutable :
```bash
chmod +x Msty-*.AppImage
./Msty-*.AppImage
```

**Snap (Ubuntu/Debian)**
```bash
sudo snap install msty
```

---

## ‚öôÔ∏è Configuration de Msty

### 1Ô∏è‚É£ Lancer Msty

Ouvrez l'application Msty sur votre PC.

---

### 2Ô∏è‚É£ Localiser les Param√®tres

La configuration peut varier selon la version. Cherchez dans :

**Option A : Menu Principal**
- **File** ‚Üí **Settings** ou **Preferences**
- **Tools** ‚Üí **Options**
- **Msty** ‚Üí **Preferences** (macOS)

**Option B : Ic√¥nes dans l'Interface**
- Cherchez : ‚öôÔ∏è (roue dent√©e)
- Ou : ‚ãÆ (trois points)
- Ou : ‚ò∞ (hamburger menu)

**Option C : Raccourci Clavier**
- Windows/Linux : `Ctrl + ,` (virgule)
- macOS : `Cmd + ,` (virgule)

---

### 3Ô∏è‚É£ Configurer la Connexion Ollama

Dans les param√®tres, cherchez une section nomm√©e :
- **"Ollama"**
- **"Server"**
- **"API Configuration"**
- **"Connection Settings"**

**Entrez l'URL de votre serveur :**

**Si vous utilisez la M√©thode 1 (Acc√®s Direct) :**
```
http://192.168.1.108:11434
```
(Remplacez `192.168.1.108` par l'IP de VOTRE serveur Z800)

**Si vous utilisez la M√©thode 2 (Tunnel SSH) :**
```
http://localhost:11434
```
(Assurez-vous que le tunnel SSH est actif)

---

### 4Ô∏è‚É£ Tester la Connexion

Cherchez un bouton :
- **"Test Connection"**
- **"Verify"**
- **"Check Server"**

Cliquez dessus.

**‚úÖ Succ√®s :**
- Message : "Connection successful"
- Liste des mod√®les appara√Æt
- Vous voyez "mistral:latest"

**‚ùå √âchec :**
- V√©rifiez l'URL (pas d'espace, bon port)
- V√©rifiez que le serveur Ollama tourne
- V√©rifiez le firewall (√âtape 3)

---

### 5Ô∏è‚É£ S√©lectionner Mistral

1. Dans l'interface principale de Msty
2. Cherchez une liste d√©roulante ou menu de s√©lection de mod√®le
3. S√©lectionnez **"mistral:latest"** ou **"mistral"**

---

## üí¨ Utilisation de Msty

### D√©marrer une Conversation

1. **Nouvelle conversation :**
   - Cliquez sur **"+ New Chat"** ou **"New Conversation"**
   
2. **Tapez votre message :**
   ```
   Bonjour ! Peux-tu te pr√©senter en fran√ßais ?
   ```

3. **Envoyez** (bouton ou `Entr√©e`)

**‚úÖ Mistral devrait r√©pondre directement depuis votre serveur Z800 !**

---

### Fonctionnalit√©s Utiles

#### Historique des Conversations
- Toutes vos conversations sont sauvegard√©es localement
- Acc√©dez-y via la barre lat√©rale

#### Param√®tres de G√©n√©ration
Vous pouvez ajuster :
- **Temperature** : Cr√©ativit√© (0.1 = pr√©cis, 1.0 = cr√©atif)
- **Max Tokens** : Longueur maximale de la r√©ponse
- **Top-p** : Diversit√© des r√©ponses

#### System Prompt (Optionnel)
D√©finissez un comportement par d√©faut :
```
Tu es un assistant expert en programmation Python.
```

#### Copier/Exporter
- Copiez les r√©ponses
- Exportez les conversations en Markdown

---

## üîß Configuration Avanc√©e (Fichier Config)

Si vous ne trouvez pas les param√®tres dans l'interface, √©ditez le fichier de configuration :

### Windows
```
C:\Users\VotreNom\AppData\Roaming\Msty\config.json
```

### macOS
```
~/Library/Application Support/Msty/config.json
```

### Linux
```
~/.config/Msty/config.json
```

**Ouvrez avec un √©diteur de texte et cherchez :**
```json
{
  "ollamaUrl": "http://localhost:11434",
  ...
}
```

**Modifiez en :**
```json
{
  "ollamaUrl": "http://192.168.1.108:11434",
  ...
}
```

**Sauvegardez et relancez Msty.**

---

## üß™ Tests et Validation

### Test 1 : Conversation Simple

**Vous :**
```
Explique-moi ce qu'est un LLM en une phrase.
```

**Mistral devrait r√©pondre :**
```
Un LLM (Large Language Model) est un mod√®le d'intelligence artificielle 
entra√Æn√© sur d'√©normes quantit√©s de texte pour comprendre et g√©n√©rer 
du langage naturel de mani√®re coh√©rente.
```

---

### Test 2 : Code

**Vous :**
```
√âcris une fonction Python qui calcule la suite de Fibonacci.
```

**Mistral devrait g√©n√©rer du code :**
```python
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
```

---

### Test 3 : Plusieurs Langues

**Vous :**
```
Hello! Can you introduce yourself in English?
```

**Mistral devrait r√©pondre en anglais.**

---

## üìä Monitoring de l'Utilisation

### Depuis le Serveur Z800

**V√©rifier que les requ√™tes arrivent :**
```bash
# Voir les logs en temps r√©el
sudo journalctl -u ollama -f

# Vous devriez voir des lignes comme :
# [GIN] 2026/01/28 - 12:34:56 | 200 | 2.5s | 192.168.1.147 | POST "/api/generate"
```

**V√©rifier l'utilisation GPU :**
```bash
# Pendant qu'une requ√™te est en cours
nvidia-smi

# Vous devriez voir :
# GPU-Util : 90-100%
# Memory-Usage : ~4-6 GB
```

---

## üé® Alternatives √† Msty

Si Msty ne vous convient pas, essayez :

### Jan (Open Source)
**Installation :**
- https://jan.ai
- Interface similaire √† ChatGPT
- Support de multiples backends

**Configuration :**
1. T√©l√©charger et installer
2. Settings ‚Üí Ollama Server
3. URL : `http://192.168.1.108:11434`

---

### LM Studio
**Installation :**
- https://lmstudio.ai
- Tr√®s complet, interface professionnelle
- Gestion de mod√®les int√©gr√©e

**Configuration :**
1. T√©l√©charger et installer
2. Preferences ‚Üí Remote Server
3. URL : `http://192.168.1.108:11434`

---

### Enchanted (macOS/iOS)
**Installation :**
- https://enchanted.to
- Natif Apple, tr√®s rapide
- Synchronisation iCloud

---

## ‚úÖ Checklist de Validation

- [ ] Msty install√© sur votre PC
- [ ] Configuration serveur Ollama ajout√©e
- [ ] Test de connexion r√©ussi
- [ ] Mod√®le "mistral" visible dans la liste
- [ ] Premi√®re conversation fonctionnelle
- [ ] R√©ponses fluides et rapides
- [ ] Historique sauvegard√©

---

## üÜò D√©pannage

### Probl√®me : "Cannot connect to server"

**V√©rifications :**
```bash
# Sur le serveur Z800
sudo systemctl status ollama
sudo ss -tlnp | grep 11434

# Depuis votre PC
ping 192.168.1.108
curl http://192.168.1.108:11434/api/tags
```

**Solutions :**
- V√©rifiez l'URL dans Msty (pas d'espace, bon port)
- V√©rifiez le firewall (√âtape 3)
- Si tunnel SSH : v√©rifiez qu'il est actif

---

### Probl√®me : Mod√®les ne s'affichent pas

**V√©rifications :**
```bash
# Sur le serveur
ollama list

# L'API doit retourner les mod√®les
curl http://192.168.1.108:11434/api/tags
```

**Solution :**
- Red√©marrez Msty
- V√©rifiez que Mistral est bien t√©l√©charg√©
- Reconnectez le serveur dans Msty

---

### Probl√®me : R√©ponses tr√®s lentes

**V√©rifications :**
```bash
# Sur le serveur - v√©rifier GPU
nvidia-smi

# V√©rifier la charge syst√®me
htop
```

**Causes possibles :**
- GPU non utilis√©e (rev√©rifiez les pilotes NVIDIA)
- RAM insuffisante (v√©rifiez le swap)
- Autre processus utilise la GPU
- R√©seau lent (si acc√®s distant)

---

### Probl√®me : Msty ne trouve pas le fichier config

**Cr√©ez-le manuellement :**

**Windows :**
```powershell
mkdir "$env:APPDATA\Msty"
echo '{"ollamaUrl":"http://192.168.1.108:11434"}' > "$env:APPDATA\Msty\config.json"
```

**macOS/Linux :**
```bash
mkdir -p ~/.config/Msty
echo '{"ollamaUrl":"http://192.168.1.108:11434"}' > ~/.config/Msty/config.json
```

---

## üìù R√©sum√©

√Ä la fin de cette √©tape :

```
‚úÖ PC Client configur√©
‚úÖ Msty install√© et connect√© au Z800
‚úÖ Interface graphique moderne
‚úÖ Conversations avec Mistral fonctionnelles
‚úÖ Historique local sauvegard√©

Architecture compl√®te :
PC (Msty) ‚Üê‚Üí R√©seau ‚Üê‚Üí Z800 (Ollama + Mistral)
```

---

## ‚û°Ô∏è Prochaines √âtapes (Optionnelles)

### Pour aller plus loin :

1. **[Configuration RAG](05-configuration-rag.md)** - Interroger vos documents
2. **[Fine-tuning Setup](06-finetuning-setup.md)** - Personnaliser Mistral
3. **[Mod√®les Additionnels](07-modeles-additionnels.md)** - T√©l√©charger d'autres LLMs

---

## üéâ F√©licitations !

Vous avez maintenant une stack LLM compl√®te et fonctionnelle :

```
‚úÖ Serveur Z800 avec Ollama natif
‚úÖ Mistral 7B op√©rationnel
‚úÖ Acc√®s r√©seau configur√© et s√©curis√©
‚úÖ Interface graphique Msty
‚úÖ GPU Quadro P4000 utilis√©e √† 100%
‚úÖ 0 overhead Docker
‚úÖ Performance maximale
```

**Profitez de votre assistant IA local et priv√© ! üöÄ**

---

**üí° Astuce :** Pour ajouter d'autres PCs, retournez √† l'√âtape 3 et ajoutez simplement leur IP dans le firewall UFW.
