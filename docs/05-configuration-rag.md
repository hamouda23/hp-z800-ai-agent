# √âtape 5 : Configuration RAG (Retrieval Augmented Generation)

## üéØ Objectif

Configurer le RAG pour permettre √† Mistral d'interroger vos propres documents (PDF, DOCX, TXT, etc.) et de r√©pondre en se basant sur leur contenu.

## ‚è±Ô∏è Dur√©e Estim√©e

20-30 minutes

## üìã Pr√©requis

- ‚úÖ Ollama install√© et fonctionnel
- ‚úÖ Mistral t√©l√©charg√©
- ‚úÖ Python 3.8+ disponible (via Conda)
- ‚úÖ Acc√®s r√©seau configur√©

---

## üß† Qu'est-ce que le RAG ?

**RAG (Retrieval Augmented Generation)** permet √† un LLM de :
1. **R√©cup√©rer** des informations pertinentes depuis vos documents
2. **Augmenter** le contexte avec ces informations
3. **G√©n√©rer** une r√©ponse bas√©e sur vos documents

### Cas d'Usage
- üìö Interroger votre documentation technique
- üìÑ Analyser des rapports/articles
- üìã Extraire des informations de contrats
- üîç Rechercher dans vos notes personnelles
- üìä Analyser des donn√©es textuelles

---

## üèóÔ∏è Architecture RAG

```
[Vos Documents]
     ‚Üì
[Extraction Texte]
     ‚Üì
[D√©coupage en Chunks]
     ‚Üì
[G√©n√©ration Embeddings] ‚Üê nomic-embed-text
     ‚Üì
[Base Vectorielle]
     ‚Üì
[Question Utilisateur]
     ‚Üì
[Recherche Similarit√©]
     ‚Üì
[Context + Question] ‚Üí [Mistral] ‚Üí [R√©ponse]
```

---

## üì¶ √âtape 1 : T√©l√©charger le Mod√®le d'Embeddings

```bash
# T√©l√©charger nomic-embed-text (~274 MB)
ollama pull nomic-embed-text

# V√©rifier
ollama list
```

**‚úÖ R√©sultat attendu :**
```
NAME                   ID              SIZE
mistral:latest         xxx...          4.1 GB
nomic-embed-text:latest xxx...         274 MB
```

---

## üêç √âtape 2 : Cr√©er un Environnement Python pour RAG

```bash
# Cr√©er un nouvel environnement Conda
conda create -n rag python=3.11 -y

# Activer l'environnement
conda activate rag

# Installer les d√©pendances
pip install requests
pip install pypdf2           # Pour les PDFs
pip install python-docx      # Pour les DOCX
pip install chromadb         # Base vectorielle
pip install sentence-transformers  # Optionnel
```

---

## üìÅ √âtape 3 : Cr√©er la Structure de Projet RAG

```bash
# Cr√©er les dossiers
mkdir -p ~/hp-z800-ai-agent/rag/{documents,scripts,vectordb}

# Structure finale
# rag/
# ‚îú‚îÄ‚îÄ documents/        # Vos documents sources
# ‚îú‚îÄ‚îÄ scripts/          # Scripts Python
# ‚îî‚îÄ‚îÄ vectordb/         # Base vectorielle (ChromaDB)
```

---

## üîß √âtape 4 : Script d'Extraction de Texte

```bash
# Cr√©er le script d'extraction
cat > ~/hp-z800-ai-agent/rag/scripts/extract_text.py << 'EOF'
#!/usr/bin/env python3
"""
Extrait le texte de diff√©rents formats de documents
"""

import os
from pathlib import Path
from typing import List, Dict

def extract_from_txt(file_path: str) -> str:
    """Extrait le texte d'un fichier TXT"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

def extract_from_pdf(file_path: str) -> str:
    """Extrait le texte d'un PDF"""
    try:
        import PyPDF2
        text = ""
        with open(file_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text
    except ImportError:
        return "Erreur: PyPDF2 non install√©. pip install pypdf2"

def extract_from_docx(file_path: str) -> str:
    """Extrait le texte d'un DOCX"""
    try:
        from docx import Document
        doc = Document(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        return text
    except ImportError:
        return "Erreur: python-docx non install√©. pip install python-docx"

def extract_text(file_path: str) -> str:
    """D√©tecte le format et extrait le texte"""
    ext = Path(file_path).suffix.lower()
    
    if ext == '.txt':
        return extract_from_txt(file_path)
    elif ext == '.pdf':
        return extract_from_pdf(file_path)
    elif ext == '.docx':
        return extract_from_docx(file_path)
    else:
        return f"Format non support√©: {ext}"

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """D√©coupe le texte en chunks avec chevauchement"""
    words = text.split()
    chunks = []
    
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    
    return chunks

if __name__ == "__main__":
    # Test
    import sys
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        text = extract_text(file_path)
        print(f"Texte extrait ({len(text)} caract√®res):")
        print(text[:500] + "...")
        
        chunks = chunk_text(text)
        print(f"\nD√©coup√© en {len(chunks)} chunks")
    else:
        print("Usage: python extract_text.py <chemin_fichier>")
EOF

chmod +x ~/hp-z800-ai-agent/rag/scripts/extract_text.py
```

---

## üî¢ √âtape 5 : Script de G√©n√©ration d'Embeddings

```bash
# Cr√©er le script d'embeddings
cat > ~/hp-z800-ai-agent/rag/scripts/generate_embeddings.py << 'EOF'
#!/usr/bin/env python3
"""
G√©n√®re des embeddings avec Ollama (nomic-embed-text)
"""

import requests
import json
from typing import List

OLLAMA_URL = "http://localhost:11434"

def generate_embedding(text: str) -> List[float]:
    """G√©n√®re un embedding pour un texte"""
    response = requests.post(
        f"{OLLAMA_URL}/api/embeddings",
        json={
            "model": "nomic-embed-text",
            "prompt": text
        }
    )
    
    if response.status_code == 200:
        return response.json()['embedding']
    else:
        raise Exception(f"Erreur API: {response.status_code}")

def generate_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """G√©n√®re des embeddings pour plusieurs textes"""
    embeddings = []
    total = len(texts)
    
    for i, text in enumerate(texts, 1):
        print(f"G√©n√©ration embedding {i}/{total}...", end='\r')
        embedding = generate_embedding(text)
        embeddings.append(embedding)
    
    print(f"\n‚úÖ {total} embeddings g√©n√©r√©s")
    return embeddings

if __name__ == "__main__":
    # Test
    test_text = "Ceci est un test d'embedding avec Ollama."
    embedding = generate_embedding(test_text)
    print(f"Embedding g√©n√©r√©: dimension {len(embedding)}")
    print(f"Premiers √©l√©ments: {embedding[:5]}")
EOF

chmod +x ~/hp-z800-ai-agent/rag/scripts/generate_embeddings.py
```

---

## üìö √âtape 6 : Script RAG Complet (Simple - Sans ChromaDB)

```bash
# Version simple sans base vectorielle
cat > ~/hp-z800-ai-agent/rag/scripts/simple_rag.py << 'EOF'
#!/usr/bin/env python3
"""
RAG Simple : Recherche dans documents et g√©n√©ration de r√©ponse
"""

import requests
import json
import os
from pathlib import Path
from extract_text import extract_text, chunk_text
from generate_embeddings import generate_embedding

OLLAMA_URL = "http://localhost:11434"

def cosine_similarity(vec1, vec2):
    """Calcule la similarit√© cosinus entre deux vecteurs"""
    import math
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(b * b for b in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0
    
    return dot_product / (magnitude1 * magnitude2)

def load_documents(docs_folder: str):
    """Charge et traite tous les documents"""
    documents = []
    docs_path = Path(docs_folder)
    
    for file_path in docs_path.glob('*'):
        if file_path.suffix.lower() in ['.txt', '.pdf', '.docx']:
            print(f"Chargement: {file_path.name}")
            text = extract_text(str(file_path))
            chunks = chunk_text(text, chunk_size=500)
            
            for i, chunk in enumerate(chunks):
                documents.append({
                    'source': file_path.name,
                    'chunk_id': i,
                    'text': chunk,
                    'embedding': None  # Sera g√©n√©r√© √† la demande
                })
    
    print(f"\n‚úÖ {len(documents)} chunks charg√©s depuis {len(list(docs_path.glob('*')))} documents")
    return documents

def search_documents(query: str, documents: list, top_k: int = 3):
    """Recherche les chunks les plus pertinents"""
    print(f"\nüîç Recherche pour: '{query}'")
    
    # G√©n√©rer embedding de la question
    query_embedding = generate_embedding(query)
    
    # Calculer similarit√© avec chaque document
    results = []
    for i, doc in enumerate(documents):
        # G√©n√©rer embedding du document si pas d√©j√† fait
        if doc['embedding'] is None:
            print(f"G√©n√©ration embedding document {i+1}/{len(documents)}...", end='\r')
            doc['embedding'] = generate_embedding(doc['text'])
        
        similarity = cosine_similarity(query_embedding, doc['embedding'])
        results.append({
            'document': doc,
            'similarity': similarity
        })
    
    # Trier par similarit√©
    results.sort(key=lambda x: x['similarity'], reverse=True)
    
    print(f"\n‚úÖ Top {top_k} r√©sultats:")
    for i, result in enumerate(results[:top_k], 1):
        print(f"{i}. {result['document']['source']} (similarit√©: {result['similarity']:.3f})")
    
    return results[:top_k]

def generate_answer(query: str, context_docs: list):
    """G√©n√®re une r√©ponse avec Mistral en utilisant le contexte"""
    
    # Construire le contexte
    context = "\n\n".join([
        f"[Source: {doc['document']['source']}]\n{doc['document']['text']}"
        for doc in context_docs
    ])
    
    # Prompt avec contexte
    prompt = f"""Contexte extrait de documents:

{context}

Question: {query}

R√©ponds √† la question en te basant UNIQUEMENT sur le contexte fourni ci-dessus. Si l'information n'est pas dans le contexte, dis-le clairement."""

    # Appel √† Mistral
    print("\nüí¨ G√©n√©ration de la r√©ponse...")
    response = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={
            "model": "mistral",
            "prompt": prompt,
            "stream": False
        }
    )
    
    if response.status_code == 200:
        return response.json()['response']
    else:
        return f"Erreur API: {response.status_code}"

def rag_query(query: str, docs_folder: str, top_k: int = 3):
    """Pipeline RAG complet"""
    print("="*60)
    print("RAG Pipeline")
    print("="*60)
    
    # 1. Charger documents
    documents = load_documents(docs_folder)
    
    # 2. Rechercher documents pertinents
    relevant_docs = search_documents(query, documents, top_k)
    
    # 3. G√©n√©rer r√©ponse
    answer = generate_answer(query, relevant_docs)
    
    print("\n" + "="*60)
    print("R√âPONSE:")
    print("="*60)
    print(answer)
    print("="*60)
    
    return answer

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python simple_rag.py <question>")
        print('Exemple: python simple_rag.py "Quelle est la conclusion du rapport ?"')
        sys.exit(1)
    
    query = " ".join(sys.argv[1:])
    docs_folder = "../documents"
    
    rag_query(query, docs_folder)
EOF

chmod +x ~/hp-z800-ai-agent/rag/scripts/simple_rag.py
```

---

## üß™ √âtape 7 : Test du RAG

### 7.1 Cr√©er un Document de Test

```bash
# Cr√©er un document test
cat > ~/hp-z800-ai-agent/rag/documents/test-doc.txt << 'EOF'
Guide d'Utilisation du HP Z800

Le HP Z800 est une workstation professionnelle lanc√©e en 2009-2010.

Sp√©cifications:
- Processeurs: Dual Intel Xeon E5640 @ 2.67 GHz
- RAM: Jusqu'√† 192 GB DDR3 ECC
- GPU: Support des cartes professionnelles NVIDIA Quadro

Configuration R√©seau:
Pour configurer le r√©seau, utilisez la commande ip addr show.
Le pare-feu peut √™tre configur√© avec ufw.

Maintenance:
Il est recommand√© de nettoyer les ventilateurs tous les 6 mois.
La p√¢te thermique du CPU doit √™tre chang√©e tous les 3-4 ans.

Conclusion:
Le Z800 reste une excellente machine pour le machine learning et l'IA en 2026.
EOF
```

### 7.2 Tester l'Extraction

```bash
# Activer l'environnement
conda activate rag

# Tester extraction
cd ~/hp-z800-ai-agent/rag/scripts
python extract_text.py ../documents/test-doc.txt
```

### 7.3 Tester les Embeddings

```bash
python generate_embeddings.py
```

### 7.4 Tester le RAG Complet

```bash
# Question sur le document
python simple_rag.py "Quelles sont les sp√©cifications du HP Z800 ?"

# Autre question
python simple_rag.py "√Ä quelle fr√©quence doit-on changer la p√¢te thermique ?"
```

**‚úÖ R√©sultat attendu :**
```
==========================================================
RAG Pipeline
==========================================================
Chargement: test-doc.txt

‚úÖ 2 chunks charg√©s depuis 1 documents

üîç Recherche pour: 'Quelles sont les sp√©cifications du HP Z800 ?'
G√©n√©ration embedding document 1/2...
G√©n√©ration embedding document 2/2...

‚úÖ Top 3 r√©sultats:
1. test-doc.txt (similarit√©: 0.856)
2. test-doc.txt (similarit√©: 0.723)

üí¨ G√©n√©ration de la r√©ponse...

==========================================================
R√âPONSE:
==========================================================
Selon le document, les sp√©cifications du HP Z800 sont:
- Processeurs: Dual Intel Xeon E5640 @ 2.67 GHz
- RAM: Jusqu'√† 192 GB DDR3 ECC
- GPU: Support des cartes professionnelles NVIDIA Quadro
==========================================================
```

---

## üìä √âtape 8 : Ajouter Vos Propres Documents

```bash
# Copier vos documents dans le dossier
cp /chemin/vers/vos/documents/*.pdf ~/hp-z800-ai-agent/rag/documents/
cp /chemin/vers/vos/documents/*.docx ~/hp-z800-ai-agent/rag/documents/
# Si le PDF est dans T√©l√©chargements :
scp "$env:USERPROFILE\Downloads\Python code for Artificial Intelligence Foundations of Computational Agents.pdf" samir@192.168.1.108:~/hp-z800-ai-agent/rag/documents/
# Lancer le RAG
cd ~/hp-z800-ai-agent/rag/scripts
python simple_rag.py "Votre question ici"
```

---

## ‚úÖ Checklist de Validation

- [ ] nomic-embed-text t√©l√©charg√©
- [ ] Environnement Conda `rag` cr√©√©
- [ ] D√©pendances Python install√©es
- [ ] Structure de dossiers cr√©√©e
- [ ] Scripts Python cr√©√©s
- [ ] Test avec document simple r√©ussi
- [ ] RAG fonctionne sur vos documents

---

## üîß Scripts Utiles

### Script de Nettoyage

```bash
# Supprimer tous les embeddings en cache
rm -rf ~/hp-z800-ai-agent/rag/vectordb/*
```

### Script de Statistiques

```bash
cat > ~/hp-z800-ai-agent/rag/scripts/stats.py << 'EOF'
#!/usr/bin/env python3
import os
from pathlib import Path

docs_folder = Path("../documents")
files = list(docs_folder.glob("*"))

print(f"Documents: {len(files)}")
for f in files:
    size = f.stat().st_size / 1024
    print(f"  - {f.name}: {size:.1f} KB")
EOF

python ~/hp-z800-ai-agent/rag/scripts/stats.py
```

---

## üìä R√©sum√© RAG

√Ä la fin de cette √©tape :

```
‚úÖ Mod√®le d'embeddings (nomic-embed-text) install√©
‚úÖ Environnement Python RAG configur√©
‚úÖ Scripts d'extraction de texte
‚úÖ Scripts de g√©n√©ration d'embeddings
‚úÖ Pipeline RAG fonctionnel
‚úÖ Test avec documents r√©ussi

Capacit√©s:
üìÑ Support PDF, DOCX, TXT
üîç Recherche s√©mantique
üí¨ R√©ponses bas√©es sur vos documents
‚ö° RAG local, pas de cloud
```

---

## ‚û°Ô∏è Prochaine √âtape

**[√âtape 6 : Setup Fine-tuning](06-finetuning-setup.md)** *(√† venir)*

Pr√©parer l'environnement pour fine-tuner Mistral sur vos propres donn√©es.

---

## üÜò D√©pannage

### Probl√®me : "nomic-embed-text not found"

```bash
ollama pull nomic-embed-text
ollama list
```

### Probl√®me : Erreur PyPDF2

```bash
conda activate rag
pip install pypdf2
```

### Probl√®me : Embeddings trop lents

- Utilisez un cache (sauvegardez les embeddings)
- R√©duisez la taille des chunks
- Utilisez moins de documents

### Probl√®me : R√©ponses non pertinentes

- Augmentez `top_k` (plus de contexte)
- Am√©liorez la qualit√© des documents sources
- Ajustez la taille des chunks

---

**üéâ Vous pouvez maintenant interroger vos propres documents avec Mistral !**
