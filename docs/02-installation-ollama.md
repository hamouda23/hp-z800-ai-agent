# √âtape 2 : Installation d'Ollama (100% Native)

## üéØ Objectif

Installer Ollama en mode natif (pas Docker) pour un acc√®s GPU direct et une performance maximale.

## ‚è±Ô∏è Dur√©e Estim√©e

5-10 minutes

## üìã Pr√©requis

- ‚úÖ √âtape 1 (Pr√©paration) compl√©t√©e
- ‚úÖ Pilotes NVIDIA fonctionnels
- ‚úÖ Acc√®s sudo
- ‚úÖ Connexion Internet

---

## 2.1 Installation d'Ollama

```bash
# T√©l√©charger et installer Ollama (installation native)
curl -fsSL https://ollama.com/install.sh | sh
```

**üìù Ce que fait ce script :**
- T√©l√©charge le binaire Ollama
- L'installe dans `/usr/local/bin/ollama`
- Cr√©e un service systemd `/etc/systemd/system/ollama.service`
- Configure l'utilisateur `ollama`
- Cr√©e le dossier de donn√©es dans `/usr/share/ollama`

**‚è±Ô∏è Temps d'installation :** ~1-2 minutes

**‚úÖ R√©sultat attendu :**
```
>>> Installing ollama to /usr/local
>>> Downloading ollama-linux-amd64.tar.zst
>>> Creating ollama user...
>>> Adding ollama user to render group...
>>> Adding ollama user to video group...
>>> Creating ollama systemd service...
>>> Enabling and starting ollama service...
>>> NVIDIA GPU installed.
```

---

## 2.2 V√©rification de l'Installation

```bash
# V√©rifier la version install√©e
ollama --version
```

**‚úÖ R√©sultat attendu :**
```
ollama version is 0.15.2
```

---

## 2.3 D√©marrage du Service

```bash
# D√©marrer le service Ollama
sudo systemctl start ollama

# Activer le d√©marrage automatique au boot
sudo systemctl enable ollama

# V√©rifier le statut
sudo systemctl status ollama
```

**‚úÖ R√©sultat attendu :**
```
‚óè ollama.service - Ollama Service
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled)
     Active: active (running) since [DATE]
       Docs: https://ollama.com/docs
   Main PID: xxxxx (ollama)
      Tasks: 12
     Memory: 141.0M
```

**üîç Points √† v√©rifier :**
- ‚úÖ **Active:** `active (running)` en vert
- ‚úÖ **Loaded:** `enabled` (d√©marrage auto)
- ‚úÖ Pas de messages d'erreur en rouge

---

## 2.4 V√©rification du Port

```bash
# V√©rifier qu'Ollama √©coute sur le port 11434
sudo ss -tlnp | grep ollama
```

**‚úÖ R√©sultat attendu :**
```
LISTEN 0  4096  127.0.0.1:11434  0.0.0.0:*  users:(("ollama",pid=xxxxx,fd=3))
```

**üìù Note :** 
- Sur Ubuntu 22.04+, utilisez `ss` au lieu de `netstat`
- Pour l'instant, Ollama √©coute uniquement sur `127.0.0.1` (localhost)
- On va le configurer pour l'acc√®s distant √† l'√©tape 3

**üí° Si vous pr√©f√©rez netstat :**
```bash
# Installer net-tools (optionnel)
sudo apt install net-tools -y

# Puis utiliser
sudo netstat -tlnp | grep ollama
```

---

## 2.5 Test de l'API en Local

```bash
# Test simple de l'API
curl http://localhost:11434/api/tags
```

**‚úÖ R√©sultat attendu :**
```json
{
  "models": []
}
```

**üìù Note :** La liste est vide car aucun mod√®le n'est encore t√©l√©charg√©. C'est normal !

**Test suppl√©mentaire :**
```bash
# V√©rifier la version de l'API
curl http://localhost:11434/api/version
```

**‚úÖ R√©sultat attendu :**
```json
{
  "version": "0.15.2"
}
```

---

## 2.6 V√©rification de l'Acc√®s GPU

```bash
# V√©rifier les logs pour voir si la GPU est d√©tect√©e
sudo journalctl -u ollama -n 50 --no-pager | grep -i "gpu\|cuda"
```

**‚úÖ R√©sultat attendu :**
```
inference compute ... name=CUDA0 description="Quadro P4000" 
total="8.0 GiB" available="7.9 GiB"
```

**üéÆ Points importants :**
- ‚úÖ GPU d√©tect√©e : **Quadro P4000**
- ‚úÖ M√©moire disponible : **~7.9 GB / 8 GB**
- ‚úÖ CUDA activ√©

---

## 2.7 T√©l√©chargement du Premier Mod√®le (Mistral)

```bash
# T√©l√©charger Mistral 7B (~4.1 GB)
ollama pull mistral
```

**‚è±Ô∏è Dur√©e :** 5-10 minutes selon votre connexion Internet

**‚úÖ Progression attendue :**
```
pulling manifest
pulling 61e88e884507... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.1 GB
pulling 43070e2d4e53... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  11 KB
pulling e6836092461f... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  42 B
pulling ed11eda7790d... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  30 B
pulling f9b1e3196ecf... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 483 B
verifying sha256 digest
writing manifest
success
```

**V√©rifier les mod√®les install√©s :**
```bash
ollama list
```

**‚úÖ R√©sultat attendu :**
```
NAME              ID              SIZE      MODIFIED
mistral:latest    6577803aa9a0    4.1 GB    X minutes ago
```

---

## 2.8 Test de Mistral

```bash
# Test rapide en mode interactif
ollama run mistral "Bonjour ! Peux-tu te pr√©senter en une phrase ?"
```

**‚úÖ Mistral devrait r√©pondre en fran√ßais**

**Pour quitter le mode interactif :**
```bash
/bye
```

**Test via l'API :**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Bonjour !",
  "stream": false
}'
```

---

## 2.9 Emplacement des Fichiers

```bash
# V√©rifier o√π Ollama stocke les mod√®les
ls -lh /usr/share/ollama/.ollama/models/
```

**üìÅ Structure :**
- **Binaire :** `/usr/local/bin/ollama`
- **Service :** `/etc/systemd/system/ollama.service`
- **Mod√®les :** `/usr/share/ollama/.ollama/models/`
- **Utilisateur :** `ollama` (cr√©√© automatiquement)

**üíæ Espace disque utilis√© :**
```bash
du -sh /usr/share/ollama/
# ~4.2 GB avec Mistral
```

---

## 2.10 Configuration du Service (Visualisation)

```bash
# Voir la configuration du service systemd
sudo systemctl cat ollama
```

**üìã Configuration par d√©faut :**
```ini
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

[Install]
WantedBy=default.target
```

**üìù Note :** On va modifier cette configuration √† l'√©tape 3 pour permettre l'acc√®s distant.

---

## ‚úÖ Checklist de Validation

**Script de validation automatique :**

```bash
# Cr√©er le script (si pas d√©j√† fait)
cat > ~/hp-z800-ai-agent/scripts/validate-ollama.sh << 'EOF'
#!/bin/bash
echo "=== Validation Installation Ollama ==="
echo ""

echo -n "‚úì Ollama install√©: "
if command -v ollama &> /dev/null; then
    echo "OK ($(ollama --version))"
else
    echo "‚ùå ERREUR"
    exit 1
fi

echo -n "‚úì Service running: "
if systemctl is-active --quiet ollama; then
    echo "OK"
else
    echo "‚ùå ERREUR"
    exit 1
fi

echo -n "‚úì GPU d√©tect√©e: "
if sudo journalctl -u ollama -n 100 --no-pager | grep -q "Quadro P4000"; then
    echo "OK (Quadro P4000 - 8GB)"
else
    echo "‚ö†Ô∏è  WARNING"
fi

echo -n "‚úì Port 11434 actif: "
if sudo ss -tlnp | grep -q ":11434"; then
    echo "OK"
else
    echo "‚ùå ERREUR"
    exit 1
fi

echo -n "‚úì API fonctionnelle: "
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "OK"
else
    echo "‚ùå ERREUR"
    exit 1
fi

echo -n "‚úì Mistral install√©: "
if ollama list | grep -q "mistral"; then
    echo "OK"
else
    echo "‚ö†Ô∏è  Pas encore install√©"
fi

echo ""
echo "=== Installation Ollama Valid√©e ‚úÖ ==="
EOF

chmod +x ~/hp-z800-ai-agent/scripts/validate-ollama.sh

# Ex√©cuter
~/hp-z800-ai-agent/scripts/validate-ollama.sh
```

**Checklist manuelle :**

- [ ] `ollama --version` fonctionne
- [ ] Service Ollama actif
- [ ] Service enabled (d√©marrage auto)
- [ ] Port 11434 en √©coute (localhost)
- [ ] API r√©pond
- [ ] GPU Quadro P4000 d√©tect√©e
- [ ] Mistral t√©l√©charg√©
- [ ] Test de chat fonctionne
- [ ] Pas d'erreurs dans les logs

---

## üìä R√©sum√© de l'Installation

√Ä la fin de cette √©tape :

```
‚úÖ Ollama 0.15.2 install√© (natif, pas Docker)
‚úÖ Service actif et enabled
‚úÖ Port 11434 en √©coute (localhost)
‚úÖ GPU Quadro P4000 d√©tect√©e (7.9 GB disponible)
‚úÖ Mistral 7B t√©l√©charg√© (4.1 GB)
‚úÖ API REST fonctionnelle
‚úÖ Chat en ligne de commande op√©rationnel

üíæ Espace utilis√©: ~4.2 GB
üß† RAM service idle: ~140 MB
üéÆ GPU pr√™te pour inf√©rence
```

---

## ‚û°Ô∏è Prochaine √âtape

Une fois toutes les v√©rifications valid√©es, passez √† :

**[√âtape 3 : Configuration R√©seau](03-configuration-reseau.md)**

Vous allez configurer Ollama pour qu'il soit accessible depuis votre PC.

---

## üÜò D√©pannage

### Probl√®me : Service ne d√©marre pas

```bash
# V√©rifier les logs d√©taill√©s
sudo journalctl -u ollama -n 100 --no-pager

# V√©rifier les permissions
sudo ls -la /usr/share/ollama/

# Red√©marrer
sudo systemctl restart ollama
```

### Probl√®me : Port 11434 d√©j√† utilis√©

```bash
# Identifier le processus
sudo lsof -i :11434
# OU
sudo ss -tlnp | grep 11434

# Arr√™ter le service qui occupe le port
sudo systemctl stop nom-du-service
```

### Probl√®me : API ne r√©pond pas

```bash
# V√©rifier que le service tourne
sudo systemctl status ollama

# V√©rifier les logs
sudo journalctl -u ollama -f

# Test manuel du binaire
/usr/local/bin/ollama serve
# Ctrl+C pour arr√™ter
```

### Probl√®me : GPU non d√©tect√©e

```bash
# V√©rifier NVIDIA
nvidia-smi

# V√©rifier les logs Ollama
sudo journalctl -u ollama | grep -i cuda

# Red√©marrer Ollama
sudo systemctl restart ollama
```

### Probl√®me : T√©l√©chargement Mistral √©choue

```bash
# V√©rifier l'espace disque
df -h /usr/share/ollama/

# V√©rifier la connexion Internet
curl -I https://ollama.com

# R√©essayer
ollama pull mistral

# Utiliser un mod√®le plus petit si n√©cessaire
ollama pull llama3.2  # 2 GB au lieu de 4 GB
```

---

## üìù Commandes Utiles

```bash
# Gestion du service
sudo systemctl start ollama
sudo systemctl stop ollama
sudo systemctl restart ollama
sudo systemctl status ollama

# Logs
sudo journalctl -u ollama -f              # Temps r√©el
sudo journalctl -u ollama -n 100          # 100 derni√®res lignes
sudo journalctl -u ollama --since today   # Logs du jour

# Mod√®les
ollama list                    # Lister
ollama pull nom-modele         # T√©l√©charger
ollama rm nom-modele           # Supprimer
ollama run nom-modele          # Ex√©cuter

# Configuration
sudo systemctl cat ollama      # Voir config
sudo systemctl edit ollama     # Modifier config
```

---

**üìù Installation native r√©ussie ! Ollama tourne avec 0 overhead et acc√®s GPU direct.**
