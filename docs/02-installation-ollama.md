# Ã‰tape 2 : Installation d'Ollama (100% Native)

## ğŸ¯ Objectif

Installer Ollama en mode natif (pas Docker) pour un accÃ¨s GPU direct et une performance maximale.

## â±ï¸ DurÃ©e EstimÃ©e

5-10 minutes

## ğŸ“‹ PrÃ©requis

- âœ… Ã‰tape 1 (PrÃ©paration) complÃ©tÃ©e
- âœ… Pilotes NVIDIA fonctionnels
- âœ… AccÃ¨s sudo
- âœ… Connexion Internet

---

## 2.1 Installation d'Ollama

```bash
# TÃ©lÃ©charger et installer Ollama (installation native)
curl -fsSL https://ollama.com/install.sh | sh
```

**ğŸ“ Ce que fait ce script :**
- TÃ©lÃ©charge le binaire Ollama
- L'installe dans `/usr/local/bin/ollama`
- CrÃ©e un service systemd `/etc/systemd/system/ollama.service`
- Configure l'utilisateur `ollama`
- CrÃ©e le dossier de donnÃ©es dans `/usr/share/ollama`

**â±ï¸ Temps d'installation :** ~1-2 minutes

---

## 2.2 VÃ©rification de l'Installation

```bash
# VÃ©rifier la version installÃ©e
ollama --version

# Exemple de sortie attendue :
# ollama version is 0.x.xx
```

**ğŸ“¸ Capture :**
```bash
ollama --version > ~/hp-z800-ai-agent/logs/ollama-version.txt
```

---

## 2.3 DÃ©marrage du Service

```bash
# DÃ©marrer le service Ollama
sudo systemctl start ollama

# Activer le dÃ©marrage automatique au boot
sudo systemctl enable ollama

# VÃ©rifier le statut
sudo systemctl status ollama
```

**âœ… RÃ©sultat attendu :**
```
â— ollama.service - Ollama Service
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled; vendor preset: enabled)
     Active: active (running) since [DATE]
       Docs: https://ollama.com/docs
   Main PID: xxxxx (ollama)
      Tasks: xx
     Memory: xxx.xM
        CPU: xxxms
     CGroup: /system.slice/ollama.service
             â””â”€xxxxx /usr/local/bin/ollama serve
```

**ğŸ” Points Ã  vÃ©rifier :**
- âœ… **Active:** `active (running)` en vert
- âœ… **Loaded:** `enabled` (dÃ©marrage auto)
- âœ… Pas de messages d'erreur en rouge

**ğŸ“¸ Capture :**
```bash
sudo systemctl status ollama --no-pager > ~/hp-z800-ai-agent/logs/ollama-status.txt
```

---

## 2.4 VÃ©rification du Port

```bash
# VÃ©rifier qu'Ollama Ã©coute sur le port 11434
sudo netstat -tlnp | grep ollama
```

**âœ… RÃ©sultat attendu :**
```
tcp        0      0 127.0.0.1:11434         0.0.0.0:*               LISTEN      xxxxx/ollama
```

**ğŸ“ Note :** Pour l'instant, Ollama Ã©coute uniquement sur `127.0.0.1` (localhost). On va le configurer pour l'accÃ¨s distant Ã  l'Ã©tape 3.

---

## 2.5 Test de l'API en Local

```bash
# Test simple de l'API
curl http://localhost:11434/api/tags
```

**âœ… RÃ©sultat attendu :**
```json
{
  "models": []
}
```

**ğŸ“ Note :** La liste est vide car aucun modÃ¨le n'est encore tÃ©lÃ©chargÃ©. C'est normal !

**Test plus complet :**
```bash
# VÃ©rifier que l'API rÃ©pond
curl http://localhost:11434/api/version
```

**âœ… RÃ©sultat attendu :**
```json
{
  "version": "0.x.xx"
}
```

---

## 2.6 VÃ©rification de l'AccÃ¨s GPU

```bash
# VÃ©rifier les variables d'environnement CUDA
sudo systemctl show ollama | grep -i cuda

# VÃ©rifier les logs pour voir si GPU est dÃ©tectÃ©e
sudo journalctl -u ollama -n 50 --no-pager | grep -i gpu
sudo journalctl -u ollama -n 50 --no-pager | grep -i cuda
```

**âœ… RÃ©sultat attendu :**
- Vous devriez voir des mentions de CUDA ou GPU dans les logs
- Pas de messages d'erreur concernant CUDA

**ğŸ“¸ Capture :**
```bash
sudo journalctl -u ollama -n 100 --no-pager > ~/hp-z800-ai-agent/logs/ollama-logs-initial.txt
```

---

## 2.7 Emplacement des Fichiers

```bash
# VÃ©rifier oÃ¹ Ollama stocke les modÃ¨les
ls -la /usr/share/ollama/.ollama/

# Structure attendue :
# models/     <- Les modÃ¨les tÃ©lÃ©chargÃ©s seront ici
```

**ğŸ’¾ Taille des modÃ¨les Ã  prÃ©voir :**
- Mistral 7B : ~4.1 GB
- nomic-embed-text : ~274 MB
- Autres modÃ¨les : variable

**âš ï¸ Important :** VÃ©rifiez que vous avez au moins **20 GB libres** sur la partition qui contient `/usr/share/ollama/`

```bash
# VÃ©rifier l'espace disponible
df -h /usr/share/ollama/
```

---

## 2.8 Configuration du Service (Visualisation)

```bash
# Voir la configuration du service systemd
sudo systemctl cat ollama
```

**ğŸ“‹ Fichier de configuration par dÃ©faut :**
```ini
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

[Install]
WantedBy=default.target
```

**ğŸ“ Note :** On va modifier cette configuration Ã  l'Ã©tape 3 pour permettre l'accÃ¨s distant.

---

## 2.9 VÃ©rification des Logs en Temps RÃ©el

```bash
# Suivre les logs en direct
sudo journalctl -u ollama -f
```

**Appuyez sur Ctrl+C pour arrÃªter**

**ğŸ“ Ã€ observer :**
- Aucun message d'erreur
- Service qui tourne normalement
- Pas de crash ou redÃ©marrage

---

## 2.10 Test de RedÃ©marrage

```bash
# RedÃ©marrer le service
sudo systemctl restart ollama

# Attendre 2 secondes
sleep 2

# VÃ©rifier qu'il a redÃ©marrÃ© correctement
sudo systemctl status ollama

# Tester l'API Ã  nouveau
curl http://localhost:11434/api/tags
```

**âœ… Si tout fonctionne :** Le service redÃ©marre sans erreur et l'API rÃ©pond.

---

## âœ… Checklist de Validation

Avant de passer Ã  l'Ã©tape suivante, vÃ©rifiez :

```bash
# Script de validation automatique
cat > ~/hp-z800-ai-agent/logs/validate-ollama.sh << 'EOF'
#!/bin/bash
echo "=== Validation Installation Ollama ==="
echo ""

# 1. Version Ollama
echo -n "âœ“ Ollama installÃ©: "
if command -v ollama &> /dev/null; then
    echo "OK ($(ollama --version))"
else
    echo "âŒ ERREUR"
    exit 1
fi

# 2. Service actif
echo -n "âœ“ Service running: "
if systemctl is-active --quiet ollama; then
    echo "OK"
else
    echo "âŒ ERREUR"
    exit 1
fi

# 3. Service enabled
echo -n "âœ“ Service enabled: "
if systemctl is-enabled --quiet ollama; then
    echo "OK"
else
    echo "âš ï¸  WARNING (pas de dÃ©marrage auto)"
fi

# 4. Port 11434 Ã©coute
echo -n "âœ“ Port 11434 actif: "
if sudo netstat -tlnp | grep -q ":11434"; then
    echo "OK"
else
    echo "âŒ ERREUR"
    exit 1
fi

# 5. API rÃ©pond
echo -n "âœ“ API fonctionnelle: "
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "OK"
else
    echo "âŒ ERREUR"
    exit 1
fi

# 6. GPU dÃ©tectÃ©e par systÃ¨me
echo -n "âœ“ GPU accessible: "
if nvidia-smi > /dev/null 2>&1; then
    echo "OK"
else
    echo "âš ï¸  WARNING"
fi

echo ""
echo "=== Installation Ollama ValidÃ©e âœ… ==="
echo ""
echo "Prochaine Ã©tape: Configuration rÃ©seau pour accÃ¨s distant"
EOF

chmod +x ~/hp-z800-ai-agent/logs/validate-ollama.sh
~/hp-z800-ai-agent/logs/validate-ollama.sh
```

**Checklist manuelle :**

- [ ] `ollama --version` fonctionne
- [ ] Service Ollama actif (`systemctl status ollama`)
- [ ] Service enabled (dÃ©marrage auto)
- [ ] Port 11434 en Ã©coute
- [ ] API rÃ©pond (`curl http://localhost:11434/api/tags`)
- [ ] nvidia-smi dÃ©tecte la GPU
- [ ] Pas d'erreurs dans les logs
- [ ] Au moins 20 GB libres pour les modÃ¨les

---

## ğŸ“Š RÃ©sumÃ© de l'Installation

Ã€ la fin de cette Ã©tape, vous avez :

```
Ollama Installation:
â”œâ”€â”€ Binaire: /usr/local/bin/ollama âœ…
â”œâ”€â”€ Service: ollama.service (active & enabled) âœ…
â”œâ”€â”€ Port: 11434 (localhost uniquement) âœ…
â”œâ”€â”€ API: Fonctionnelle âœ…
â”œâ”€â”€ ModÃ¨les: Dossier prÃªt (vide) âœ…
â”œâ”€â”€ GPU: DÃ©tectÃ©e par le systÃ¨me âœ…
â””â”€â”€ Logs: Aucune erreur âœ…

Utilisation RAM actuelle: ~50-100 MB (service idle)
```

---

## â¡ï¸ Prochaine Ã‰tape

Une fois toutes les vÃ©rifications validÃ©es, passez Ã  :

**[Ã‰tape 3 : Configuration RÃ©seau](03-configuration-reseau.md)**

Vous allez configurer Ollama pour qu'il soit accessible depuis votre PC.

---

## ğŸ†˜ DÃ©pannage

### ProblÃ¨me : Service ne dÃ©marre pas

```bash
# VÃ©rifier les logs dÃ©taillÃ©s
sudo journalctl -u ollama -n 100 --no-pager

# VÃ©rifier les permissions
sudo ls -la /usr/share/ollama/

# RÃ©installer
sudo systemctl stop ollama
sudo rm -f /usr/local/bin/ollama
curl -fsSL https://ollama.com/install.sh | sh
```

### ProblÃ¨me : Port 11434 dÃ©jÃ  utilisÃ©

```bash
# Identifier le processus
sudo lsof -i :11434

# ArrÃªter le service qui occupe le port
sudo systemctl stop nom-du-service

# RedÃ©marrer Ollama
sudo systemctl restart ollama
```

### ProblÃ¨me : API ne rÃ©pond pas

```bash
# VÃ©rifier que le service tourne
sudo systemctl status ollama

# VÃ©rifier les logs
sudo journalctl -u ollama -f

# Test manuel
/usr/local/bin/ollama serve
# Ctrl+C pour arrÃªter, puis redÃ©marrer le service
```

### ProblÃ¨me : GPU non dÃ©tectÃ©e

```bash
# VÃ©rifier NVIDIA
nvidia-smi

# RÃ©installer les pilotes si nÃ©cessaire
sudo ubuntu-drivers autoinstall
sudo reboot

# AprÃ¨s reboot, redÃ©marrer Ollama
sudo systemctl restart ollama
```

---

## ğŸ“ Notes Importantes

### DiffÃ©rences Native vs Docker

**âœ… Avantages installation native :**
- AccÃ¨s GPU direct (pas de NVIDIA Container Toolkit)
- 0 overhead RAM (~800 MB Ã©conomisÃ©s)
- DÃ©marrage plus rapide
- Logs systÃ¨me natifs
- Mise Ã  jour simple (`curl ... | sh`)

**âš ï¸ Ã€ retenir :**
- Les modÃ¨les sont dans `/usr/share/ollama/.ollama/models/`
- Le service tourne sous l'utilisateur `ollama`
- Configuration dans `/etc/systemd/system/ollama.service`

### Commandes Utiles

```bash
# DÃ©marrer/ArrÃªter/RedÃ©marrer
sudo systemctl start ollama
sudo systemctl stop ollama
sudo systemctl restart ollama

# Voir les logs
sudo journalctl -u ollama -f          # Temps rÃ©el
sudo journalctl -u ollama -n 100      # 100 derniÃ¨res lignes
sudo journalctl -u ollama --since today  # Logs du jour

# VÃ©rifier la configuration
sudo systemctl cat ollama
sudo systemctl show ollama

# DÃ©sactiver le dÃ©marrage auto (si besoin)
sudo systemctl disable ollama
```

---

**ğŸ“ Sauvegardez bien tous les logs dans `~/hp-z800-ai-agent/logs/` pour rÃ©fÃ©rence future !**
