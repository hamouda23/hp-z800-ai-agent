# hp-z800-ai-agent

ğŸ¤– Stack LLM 100% Native sur HP Z800 : Ollama + Client Distant + RAG + Fine-tuning

## ğŸ¯ Philosophie du Projet

**Installation 100% NATIVE** - Pas de Docker, contrÃ´le total, performance maximale.

### Pourquoi Native ?

- âœ… **0 overhead RAM** - Toute la RAM disponible pour les modÃ¨les
- âœ… **AccÃ¨s GPU direct** - Performance maximale pour fine-tuning
- âœ… **ContrÃ´le total du Swap** - Crucial avec 12 GB RAM
- âœ… **SimplicitÃ©** - Pas de complexitÃ© Docker/containers
- âœ… **Ã‰conomie** - 1.5 GB RAM Ã©conomisÃ©s vs Docker

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Votre PC (Interface Utilisateur)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Msty / Jan (Interface graphique)      â”‚
â”‚  â€¢ Scripts Python (automatisation)       â”‚
â”‚  â€¢ VS Code Remote (dÃ©veloppement)        â”‚
â”‚    â†“                                      â”‚
â”‚    â””â”€â”€â†’ API HTTP: 192.168.x.x:11434      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ API REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HP Z800 (Backend 100% Natif)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                           â”‚
â”‚  ğŸ”· Ollama Native (Port 11434)           â”‚
â”‚     â”œâ”€ mistral (Chat principal)          â”‚
â”‚     â”œâ”€ nomic-embed-text (RAG)            â”‚
â”‚     â””â”€ AccÃ¨s GPU direct                  â”‚
â”‚                                           â”‚
â”‚  ğŸ”· Conda Environments (Disque 2)        â”‚
â”‚     â”œâ”€ base (Python 3.x)                 â”‚
â”‚     â”œâ”€ env1 (Python 3.x)                 â”‚
â”‚     â”œâ”€ env2 (Python 3.x)                 â”‚
â”‚     â””â”€ finetuning (PyTorch + CUDA)       â”‚
â”‚                                           â”‚
â”‚  Resources:                               â”‚
â”‚  â€¢ RAM disponible: ~11 GB (vs 9.5 Docker)â”‚
â”‚  â€¢ GPU: Quadro P4000 8GB (100% perfs)    â”‚
â”‚  â€¢ Swap: 8 GB configurÃ©                  â”‚
â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š SpÃ©cifications du Serveur

| Composant | DÃ©tails |
|-----------|---------|
| **ModÃ¨le** | HP Z800 Workstation (2009-2010) |
| **CPU** | 2Ã— Intel Xeon E5640 @ 2.67 GHz (8 cÅ“urs, 16 threads) |
| **RAM** | 12 GB DDR3 ECC |
| **GPU** | NVIDIA Quadro P4000 (8 GB GDDR5, 1792 CUDA cores) |
| **OS** | Ubuntu Server 22.04 LTS (noyau 6.8.0-40-generic) |
| **Storage** | Disque 1: SystÃ¨me / Disque 2: Conda & ML |
| **Pilotes** | NVIDIA 535+ avec CUDA 12.x |

## ğŸš€ Installation Rapide

### Sur le Serveur HP Z800

```bash
# Cloner le projet
git clone https://github.com/VOTRE-USERNAME/hp-z800-ai-agent.git
cd hp-z800-ai-agent

# Lancer l'installation complÃ¨te
chmod +x scripts/install-all-native.sh
./scripts/install-all-native.sh

# L'IP du serveur sera affichÃ©e Ã  la fin
```

### Sur Votre PC

**Option A : Client Graphique (RecommandÃ©)**

```bash
# Windows
winget install Msty

# macOS
brew install --cask msty

# Linux
# TÃ©lÃ©charger depuis https://msty.app
```

Puis configurer : `http://IP-DU-Z800:11434`

**Option B : Scripts Python**

```bash
pip install requests
python examples/chat-client.py
```

## ğŸ“š Installation Manuelle (Ã‰tape par Ã‰tape)

Pour une installation contrÃ´lÃ©e, suivez ces guides dans l'ordre :

1. **[PrÃ©paration SystÃ¨me](docs/01-preparation.md)** - VÃ©rifications et prÃ©requis
2. **[Installation Ollama](docs/02-installation-ollama.md)** - Backend LLM natif
3. **[Configuration RÃ©seau](docs/03-configuration-reseau.md)** - AccÃ¨s distant sÃ©curisÃ©
4. **[Installation Client PC](docs/04-client-pc.md)** - Msty, Jan ou scripts
5. **[Configuration RAG ](docs/05-configuration-rag.md)** - Documents et embeddings
6. **[RAG AvancÃ© avec ChromaDB](docs/06-rag-avance-chromadb.md)** - RAG AvancÃ©
7. **Setup Fine-tuning** *(Ã  venir)* - Environnement Conda
8. **Optimisation Swap** *(Ã  venir)* - Gestion mÃ©moire

## ğŸ’¡ Utilisation

### AccÃ¨s Ã  l'Interface Web

```
http://IP-DU-Z800:11434
```

**Depuis Msty :**
1. Ouvrir Msty
2. Settings â†’ Ollama Server â†’ `http://IP-Z800:11434`
3. SÃ©lectionner le modÃ¨le "mistral"
4. Commencer Ã  discuter !

### Upload de Documents (RAG)

*(Documentation Ã  venir)*

### API REST (pour scripts/intÃ©grations)

```bash
# GÃ©nÃ©ration de texte
curl http://IP-DU-Z800:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Explique-moi le RAG",
  "stream": false
}'
```

```python
# Client Python
import requests

def ask_mistral(prompt):
    response = requests.post(
        "http://IP-DU-Z800:11434/api/generate",
        json={"model": "mistral", "prompt": prompt, "stream": False}
    )
    return response.json()['response']

print(ask_mistral("Qu'est-ce que le machine learning?"))
```

## ğŸ“ FonctionnalitÃ©s

### Chat de Base

- âœ… Interface graphique moderne (Msty/Jan)
- âœ… Scripts Python personnalisables
- âœ… API REST compatible OpenAI
- âœ… Streaming des rÃ©ponses
- âœ… Multi-modÃ¨les

### RAG (Retrieval Augmented Generation)

*(Documentation Ã  venir)*

Interrogez vos propres documents :
- Documentation technique
- Manuels d'utilisation
- Articles de recherche
- Notes personnelles
- Bases de connaissances

### Fine-tuning

*(Documentation Ã  venir)*

Environnement Conda configurÃ© pour :
- Fine-tuner Mistral sur vos donnÃ©es
- CrÃ©er des modÃ¨les spÃ©cialisÃ©s
- EntraÃ®ner des adapters LoRA

## ğŸ“ Structure du Projet

```
hp-z800-ai-agent/
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ docs/                          # Documentation dÃ©taillÃ©e
â”‚   â”œâ”€â”€ 01-preparation.md
â”‚   â”œâ”€â”€ 02-installation-ollama.md
â”‚   â”œâ”€â”€ 03-configuration-reseau.md
â”‚   â””â”€â”€ 04-client-pc.md
â”œâ”€â”€ scripts/                       # Scripts d'installation
â”‚   â”œâ”€â”€ validate-ollama.sh
â”‚   â””â”€â”€ test-network-access.sh
â”œâ”€â”€ examples/                      # Exemples de code
â”‚   â”œâ”€â”€ chat-client.py
â”‚   â””â”€â”€ rag-example.py
â””â”€â”€ logs/                          # Logs d'installation
    â””â”€â”€ .gitkeep
```

## ğŸ› ï¸ ModÃ¨les Disponibles

| ModÃ¨le | Taille | Usage | RAM Requise |
|--------|--------|-------|-------------|
| **mistral** | 4.1 GB | Chat principal | 6 GB |
| **nomic-embed-text** | 274 MB | Embeddings RAG | 1 GB |
| llama3.2 | 2 GB | Alternative lÃ©gÃ¨re | 4 GB |
| codellama | 3.8 GB | Code generation | 6 GB |

```bash
# TÃ©lÃ©charger un nouveau modÃ¨le
ollama pull nom-du-modele

# Lister les modÃ¨les installÃ©s
ollama list
```

## ğŸ”’ SÃ©curitÃ©

### Configuration Firewall

```bash
# Autoriser uniquement votre IP
sudo ufw allow from VOTRE-IP to any port 11434
sudo ufw allow from VOTRE-IP to any port 11434
sudo ufw enable
```

### Tunnel SSH (Alternative sÃ©curisÃ©e)

```bash
# Depuis votre PC
ssh -L 11434:localhost:11434 user@IP-Z800

# Puis accÃ©dez Ã  http://localhost:11434
```

## ğŸ“Š Monitoring

```bash
# VÃ©rifier l'utilisation GPU
watch -n 1 nvidia-smi

# Logs Ollama
sudo journalctl -u ollama -f

# Statut des services
sudo systemctl status ollama
```

## ğŸ†˜ DÃ©pannage

### ProblÃ¨me : Impossible de se connecter depuis le PC

```bash
# Sur le serveur
sudo ss -tlnp | grep 11434
sudo ufw status
curl http://localhost:11434/api/tags

# Depuis le PC
ping IP-DU-Z800
```

Voir la documentation complÃ¨te dans chaque fichier `docs/`.

## ğŸ“– Ressources

- [Documentation Ollama](https://github.com/ollama/ollama)
- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Msty - Client PC](https://msty.app)
- [Jan - Client alternatif](https://jan.ai)
- [Guide Fine-tuning LLMs](https://huggingface.co/docs/transformers/training)

## ğŸ¤ Contribution

Ce projet documente l'installation 100% native d'une stack LLM sur ancien hardware workstation. 

**Objectifs** :
- Maximiser les performances avec hardware limitÃ©
- Ã‰viter la complexitÃ© Docker
- ContrÃ´le total pour fine-tuning
- Documentation dÃ©taillÃ©e pour reproductibilitÃ©

## ğŸ“ License

MIT

## âœ… Checklist d'Installation

### Sur le Serveur Z800
- [ ] Ubuntu 22.04 Ã  jour
- [ ] Pilotes NVIDIA installÃ©s
- [ ] Ollama installÃ© en natif
- [ ] ModÃ¨les tÃ©lÃ©chargÃ©s (mistral)
- [ ] Configuration rÃ©seau (0.0.0.0:11434)
- [ ] Firewall configurÃ©
- [ ] Swap optimisÃ© (8 GB)

### Sur Votre PC
- [ ] Msty installÃ©
- [ ] Connexion au serveur configurÃ©e
- [ ] Test de chat rÃ©ussi
- [ ] Scripts Python fonctionnels (optionnel)

## ğŸ¯ Roadmap

- [x] Installation Ollama native
- [x] Configuration accÃ¨s distant
- [x] Documentation client PC (Msty)
- [ ] Configuration RAG
- [ ] Setup fine-tuning complet
- [ ] Datasets exemple
- [ ] Guide LoRA/QLoRA
- [ ] Scripts d'entraÃ®nement
- [ ] Monitoring avancÃ©

---

**âš¡ Fait avec passion sur un HP Z800 de 2009 - Preuve que le vieux hardware peut encore servir !**
