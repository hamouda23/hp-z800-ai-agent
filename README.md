# hp-z800-ai-agent

## ğŸ¯ Objectif du Projet

ğŸ¤– Stack LLM 100% Native sur HP Z800 : Ollama + Client Distant + RAG + Fine-tuning

## ğŸ¯ Philosophie du Projet

**Installation 100% NATIVE** - Pas de Docker, contrÃ´le total, performance maximale.

### Pourquoi Native ?

- âœ… **0 overhead RAM** - Toute la RAM disponible pour les modÃ¨les
- âœ… **AccÃ¨s GPU direct** - Performance maximale pour fine-tuning
- âœ… **ContrÃ´le total du Swap** - Crucial avec 12 GB RAM
- âœ… **SimplicitÃ©** - Pas de complexitÃ© Docker/containers
- âœ… **Ã‰conomie** - 1.5 GB RAM Ã©conomisÃ©s vs Docker

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Votre PC (Interface Utilisateur)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Msty / Jan (Interface graphique)      â”‚
â”‚  â€¢ Scripts Python (automatisation)       â”‚
â”‚  â€¢ VS Code Remote (dÃ©veloppement)        â”‚
â”‚    â†“                                      â”‚
â”‚    â””â”€â”€â†’ API HTTP: 192.168.x.x:11434      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ API REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HP Z800 (Backend 100% Natif)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                           â”‚
â”‚  ğŸ”· Ollama Native (Port 11434)           â”‚
â”‚     â”œâ”€ mistral (Chat principal)          â”‚
â”‚     â”œâ”€ nomic-embed-text (RAG)            â”‚
â”‚     â””â”€ AccÃ¨s GPU direct                  â”‚
â”‚                                           â”‚
â”‚  ğŸ”· Conda Environments (Disque 2)        â”‚
â”‚     â”œâ”€ base (Python 3.x)                 â”‚
â”‚     â”œâ”€ env1 (Python 3.x)                 â”‚
â”‚     â”œâ”€ env2 (Python 3.x)                 â”‚
â”‚     â””â”€ finetuning (PyTorch + CUDA)       â”‚
â”‚                                           â”‚
â”‚  Resources:                               â”‚
â”‚  â€¢ RAM disponible: ~11 GB (vs 9.5 Docker)â”‚
â”‚  â€¢ GPU: Quadro P4000 8GB (100% perfs)    â”‚
â”‚  â€¢ Swap: 8 GB configurÃ©                  â”‚
â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š SpÃ©cifications du Serveur

| Composant | DÃ©tails |
|-----------|---------|
| **ModÃ¨le** | HP Z800 Workstation (2009-2010) |
| **CPU** | 2Ã— Intel Xeon E5640 @ 2.67 GHz (8 cÅ“urs, 16 threads) |
| **RAM** | 12 GB DDR3 ECC |
| **GPU** | NVIDIA Quadro P4000 (8 GB GDDR5, 1792 CUDA cores) |
| **OS** | Ubuntu Server 22.04 LTS (noyau 6.8.0-40-generic) |
| **Storage** | Disque 1: SystÃ¨me / Disque 2: Conda & ML |
| **Pilotes** | NVIDIA 535+ avec CUDA 12.x |

## ğŸš€ Installation Rapide

### Sur le Serveur HP Z800

```bash
# Cloner le projet
git clone https://github.com/VOTRE-USERNAME/hp-z800-ai-agent.git
cd hp-z800-ai-agent

# Lancer l'installation complÃ¨te
chmod +x scripts/install-all-native.sh
./scripts/install-all-native.sh

# L'IP du serveur sera affichÃ©e Ã  la fin
```

### Sur Votre PC

**Option A : Client Graphique (RecommandÃ©)**

```bash
# Windows
winget install Msty

# macOS
brew install --cask msty

# Linux
# TÃ©lÃ©charger depuis https://msty.app
```

Puis configurer : `http://IP-DU-Z800:11434`

**Option B : Scripts Python**

```bash
pip install requests
python examples/chat-client.py
```

## ğŸ“š Installation Manuelle (Ã‰tape par Ã‰tape)

Pour une installation contrÃ´lÃ©e, suivez ces guides dans l'ordre :

1. **[PrÃ©paration SystÃ¨me](docs/01-preparation.md)** - VÃ©rifications et prÃ©requis
2. **[Installation Ollama](docs/02-installation-ollama.md)** - Backend LLM natif
3. **[Configuration RÃ©seau](docs/03-configuration-reseau.md)** - AccÃ¨s distant sÃ©curisÃ©
4. **[TÃ©lÃ©chargement ModÃ¨les](docs/04-client-pc.md)** - Msty
5. **[Installation Client PC](docs/05-client-pc.md)** - Msty, Jan ou scripts
6. **[Configuration RAG](docs/06-configuration-rag.md)** - Documents et embeddings
7. **[Setup Fine-tuning](docs/07-finetuning-setup.md)** - Environnement Conda
8. **[Optimisation Swap](docs/08-optimisation-swap.md)** - Gestion mÃ©moire

## ğŸ’¡ Utilisation

### Depuis Votre PC - Interface Graphique (Msty/Jan)

1. Ouvrir Msty
2. Settings â†’ Ollama Server â†’ `http://IP-Z800:11434`
3. SÃ©lectionner le modÃ¨le "mistral"
4. Commencer Ã  chatter !

### Depuis Votre PC - Scripts Python

```python
# chat-simple.py
import requests

OLLAMA_URL = "http://192.168.1.XXX:11434"  # Votre IP Z800

def ask_mistral(prompt):
    response = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={
            "model": "mistral",
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()['response']

# Utilisation
result = ask_mistral("Qu'est-ce que le machine learning?")
print(result)
```

### API REST Directe

```bash
# Depuis votre PC
curl http://IP-Z800:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Bonjour!",
  "stream": false
}'
```

## ğŸ“ FonctionnalitÃ©s

### Chat de Base

- âœ… Interface graphique moderne (Msty/Jan)
- âœ… Scripts Python personnalisables
- âœ… API REST compatible OpenAI
- âœ… Streaming des rÃ©ponses
- âœ… Multi-modÃ¨les

### RAG (Retrieval Augmented Generation)

Interrogez vos documents :

```python
# rag-query.py
import requests

def rag_query(question, context):
    prompt = f"""Contexte: {context}
    
Question: {question}

RÃ©ponds en te basant uniquement sur le contexte fourni."""
    
    response = requests.post(
        f"{OLLAMA_URL}/api/generate",
        json={"model": "mistral", "prompt": prompt, "stream": False}
    )
    return response.json()['response']
```

Formats supportÃ©s :
- PDF, DOCX, TXT, MD, CSV
- Extraction automatique avec Python
- Embeddings avec nomic-embed-text

### Fine-tuning

Environnement Conda dÃ©diÃ© pour :
- Fine-tuner Mistral sur vos donnÃ©es
- CrÃ©er des modÃ¨les spÃ©cialisÃ©s
- EntraÃ®ner des adapters LoRA/QLoRA
- Utiliser tout le RAM + Swap (jusqu'Ã  20 GB)

```bash
# Activer l'environnement
conda activate finetuning

# Voir docs/07-finetuning-setup.md
```

## ğŸ“ Structure du Projet

```
hp-z800-ai-agent/
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ docs/                          # Documentation dÃ©taillÃ©e
â”‚   â”œâ”€â”€ 01-preparation.md
â”‚   â”œâ”€â”€ 02-installation-ollama.md
â”‚   â”œâ”€â”€ 03-configuration-reseau.md
â”‚   â”œâ”€â”€ 04-download-models.md
â”‚   â”œâ”€â”€ 05-client-pc.md
â”‚   â”œâ”€â”€ 06-configuration-rag.md
â”‚   â”œâ”€â”€ 07-finetuning-setup.md
â”‚   â”œâ”€â”€ 08-optimisation-swap.md
â”‚   â””â”€â”€ troubleshooting.md
â”œâ”€â”€ scripts/                       # Scripts d'installation
â”‚   â”œâ”€â”€ install-all-native.sh     # Installation complÃ¨te
â”‚   â”œâ”€â”€ install-ollama.sh         # Ollama seul
â”‚   â”œâ”€â”€ configure-network.sh      # Configuration rÃ©seau
â”‚   â”œâ”€â”€ setup-swap.sh             # Configuration swap
â”‚   â”œâ”€â”€ setup-finetuning-env.sh   # Env Conda fine-tuning
â”‚   â”œâ”€â”€ test-gpu.sh               # Test NVIDIA
â”‚   â””â”€â”€ monitor.sh                # Monitoring ressources
â”œâ”€â”€ examples/                      # Exemples client PC
â”‚   â”œâ”€â”€ chat-client.py            # Client CLI simple
â”‚   â”œâ”€â”€ chat-gui-streamlit.py     # Interface web lÃ©gÃ¨re
â”‚   â”œâ”€â”€ rag-pdf.py                # RAG avec PDFs
â”‚   â”œâ”€â”€ rag-documents.py          # RAG multi-documents
â”‚   â””â”€â”€ batch-processing.py       # Traitement par lots
â”œâ”€â”€ config/                        # Configurations
â”‚   â”œâ”€â”€ ollama.service            # Systemd service
â”‚   â””â”€â”€ firewall-rules.sh         # RÃ¨gles UFW
â””â”€â”€ logs/                          # Logs d'installation
    â””â”€â”€ .gitkeep
```

## ğŸ› ï¸ ModÃ¨les Disponibles

| ModÃ¨le | Taille | Usage | RAM Requise |
|--------|--------|-------|-------------|
| **mistral:7b** | 4.1 GB | Chat principal | 6 GB |
| **mistral:instruct** | 4.1 GB | Instructions | 6 GB |
| **nomic-embed-text** | 274 MB | Embeddings RAG | 1 GB |
| llama3.2 | 2 GB | Alternative lÃ©gÃ¨re | 4 GB |
| codellama:7b | 3.8 GB | GÃ©nÃ©ration code | 6 GB |
| deepseek-coder | 3.8 GB | Code spÃ©cialisÃ© | 6 GB |

```bash
# TÃ©lÃ©charger un modÃ¨le
ollama pull nom-du-modele

# Lister les modÃ¨les
ollama list

# Supprimer un modÃ¨le
ollama rm nom-du-modele
```

## ğŸ“Š Utilisation des Ressources

### Comparaison Native vs Docker

| Composant | Native | Docker | Ã‰conomie |
|-----------|--------|--------|----------|
| Ollama + Mistral | 4.1 GB | 4.5 GB | 400 MB |
| Interface | 0 MB* | 800 MB | 800 MB |
| Runtime | 0 MB | 300 MB | 300 MB |
| **Total utilisÃ©** | **4.1 GB** | **5.6 GB** | **1.5 GB** |
| **RAM disponible** | **~8 GB** | **~6.5 GB** | **+23%** |

*Interface sur votre PC, pas sur le serveur

### Configuration Optimale

```bash
# RAM physique: 12 GB
# Ollama + modÃ¨les: ~4-5 GB
# SystÃ¨me Ubuntu: ~1 GB
# Disponible: ~6-7 GB

# Swap recommandÃ©: 8 GB
# Total mÃ©moire virtuelle: 20 GB
# â†’ Suffisant pour fine-tuning !
```

## ğŸ”’ SÃ©curitÃ©

### Firewall Configuration

```bash
# Autoriser uniquement votre PC
sudo ufw allow from VOTRE-IP-PC to any port 11434
sudo ufw allow 22/tcp  # SSH
sudo ufw enable
```

### Tunnel SSH (Alternative)

```bash
# Depuis votre PC - AccÃ¨s sÃ©curisÃ© sans ouvrir de ports
ssh -L 11434:localhost:11434 user@IP-Z800

# Puis utiliser: http://localhost:11434
```

### RÃ©seau Local Uniquement

```bash
# Bind Ollama sur IP locale uniquement
# Dans /etc/systemd/system/ollama.service.d/override.conf
Environment="OLLAMA_HOST=192.168.1.XXX:11434"
```

## ğŸ“Š Monitoring

```bash
# GPU en temps rÃ©el
watch -n 1 nvidia-smi

# Utilisation ressources
./scripts/monitor.sh

# Logs Ollama
sudo journalctl -u ollama -f

# Statut service
sudo systemctl status ollama
```

## ğŸ”§ Scripts Utiles

```bash
# Test complet GPU + CUDA
./scripts/test-gpu.sh

# Configurer swap optimal
./scripts/setup-swap.sh

# CrÃ©er environnement fine-tuning
./scripts/setup-finetuning-env.sh

# Monitoring continu
./scripts/monitor.sh
```

## ğŸ†˜ DÃ©pannage

### ProblÃ¨me : Impossible de se connecter depuis le PC

```bash
# Sur le Z800 - VÃ©rifier qu'Ollama Ã©coute sur 0.0.0.0
sudo netstat -tlnp | grep 11434
# Devrait afficher: 0.0.0.0:11434

# VÃ©rifier le firewall
sudo ufw status

# Tester localement
curl http://localhost:11434/api/tags
```

### ProblÃ¨me : ModÃ¨le trop lent

```bash
# VÃ©rifier que GPU est utilisÃ©e
nvidia-smi

# VÃ©rifier les variables CUDA
env | grep CUDA

# Forcer l'utilisation GPU
export CUDA_VISIBLE_DEVICES=0
sudo systemctl restart ollama
```

### ProblÃ¨me : Manque de mÃ©moire

```bash
# VÃ©rifier swap
swapon --show

# Augmenter swap (voir docs/08-optimisation-swap.md)
./scripts/setup-swap.sh

# Utiliser un modÃ¨le plus lÃ©ger
ollama pull llama3.2  # 2GB au lieu de 4GB
```

Voir [docs/troubleshooting.md](docs/troubleshooting.md) pour plus de solutions.

## ğŸ“– Ressources

- [Documentation Ollama](https://github.com/ollama/ollama)
- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Msty - Client PC](https://msty.app)
- [Jan - Client alternatif](https://jan.ai)
- [Guide Fine-tuning LLMs](https://huggingface.co/docs/transformers/training)

## ğŸ¤ Contribution

Ce projet documente l'installation 100% native d'une stack LLM sur ancien hardware workstation. 

**Objectifs** :
- Maximiser les performances avec hardware limitÃ©
- Ã‰viter la complexitÃ© Docker
- ContrÃ´le total pour fine-tuning
- Documentation dÃ©taillÃ©e pour reproductibilitÃ©

## ğŸ“ License

MIT

## âœ… Checklist d'Installation

### Sur le Serveur Z800
- [ ] Ubuntu 22.04 Ã  jour
- [ ] Pilotes NVIDIA installÃ©s
- [ ] Ollama installÃ© en natif
- [ ] ModÃ¨les tÃ©lÃ©chargÃ©s (mistral + nomic-embed-text)
- [ ] Configuration rÃ©seau (0.0.0.0:11434)
- [ ] Firewall configurÃ©
- [ ] Swap optimisÃ© (8 GB)
- [ ] Environnement Conda fine-tuning crÃ©Ã©

### Sur Votre PC
- [ ] Msty ou Jan installÃ©
- [ ] Connexion au serveur configurÃ©e
- [ ] Test de chat rÃ©ussi
- [ ] Scripts Python fonctionnels (optionnel)

## ğŸ¯ Roadmap

- [x] Installation Ollama native
- [x] Configuration accÃ¨s distant
- [x] Documentation client PC
- [x] Configuration RAG
- [ ] Setup fine-tuning complet
- [ ] Datasets exemple
- [ ] Guide LoRA/QLoRA
- [ ] Scripts d'entraÃ®nement
- [ ] Monitoring avancÃ©

---

**âš¡ Fait avec passion sur un HP Z800 de 2009 - Preuve que le vieux hardware peut encore servir !**
